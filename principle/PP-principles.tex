\documentclass{beamer}
\usetheme{Warsaw}

\usepackage{color}
\usepackage{CJK}
\usepackage{listings}
\usepackage{url}
\usepackage{booktabs}
\usepackage{pgf}
\usepackage{multicol}
%% \usepackage{algorithm}
%% \usepackage{algorithmic}

\input{../slide-macro}

\begin{document}
\begin{CJK}{UTF8}{bsmi}

\title{Parallel Algorithm Principles}

\author{Pangfeng Liu \\ National Taiwan University}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\frametitle{Principles} There are three basic principles in improving
the efficiency of parallel computing.
\begin{itemize}
\item Even partition
\item Communication reduction
\item Efficient Implementation
\end{itemize}
\end{frame}

\section{Even Partition}

\begin{frame}
\frametitle{Partition}
\begin{itemize}
\item Partition is an essential parallel algorithm design technique.
\item As in a sequential divide-and-conquer algorithm, the problem is
  first partitioned (divided) into sub-problems.
\item Unlike a sequential divide-and-conquer algorithm, a parallel
  algorithm solves (conquers) the sub-problem {\em in parallel}.
\item Some communication may be necessary since the sub-problems may
  have dependency on each other, or may need to transfer data among
  themselves.
\item Finally the answers from individual sub-problems are combined
  into the final answer.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Partition}
\begin{itemize}
\item Partition is the first step in a divide-and-conquer algorithm.
\item One can partition the data, and the process is called ``data
  partition'';
\item Or one can partition the main loop of the computation, and it is
  called ``loop'' partition.
\item The partition has a significant impact on the overall
  performance.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Give an example of divide-and-conquer computation.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Partition Principles}
There are two important issues in partitioning.
\begin{itemize}
\item Even workload distribution
\item Proper granularity
\end{itemize}
\end{frame}

\subsection{Even Workload Distribution}

\begin{frame}
\frametitle{Even Workload Distribution}
\begin{itemize}
\item We want to distribute the workload among processors so that the
  maximum workload among processors is minimized.
\item The execution time of a parallel program is the execution time
  of the {\em slowest} processor involved, which is usually the
  processor that has the maximum workload.
\begin{itemize}
\item This is the ``makespan'' of the execution time.  Note that we
  are interested in the makespan of the execution, not the sum or
  average of the execution time of processors.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Idle v.s. Busy}
\begin{itemize}
\item Uneven distribution of workload leaves some processor idle while
  others are busy.
\item If everyone is busy all the time, then the workload is evenly
  distributed.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Workload Estimation}
\begin{itemize}
\item In order to evenly distribute the workload, one needs to
  accurately predict the workload.
\item For data parallel computation, one can associate the computation
  with the data.  If we further assume that the computation workload on
  every data is about {\em the same} then we can estimate the workload
  by counting the {\em number} of data each processor is assigned.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Workload Estimation}
\begin{itemize}
\item For task parallel computation, we must predict the workload of
  sub-problems.
\item It is difficult to estimate the workload of tasks, so profiling
  or programmer intervention is necessary.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Give an example to illustrate the importance of even workload
  distribution.
\end{itemize}
\end{frame}

\subsection{Proper Granularity}

\begin{frame}
\frametitle{Granularity}
\begin{itemize}
\item The granularity is the basic unit in partitioning.
\item For data parallel computation, it indicates the smallest chunk
  of data while assigning data chunks to processors.
\item For task parallel computation, it indicates the smallest chunk
  of task while assigning tasks to processors.
\begin{itemize}
\item Recall that we can always {\em refine} a step of our algorithm
  into finer steps.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{The Size}
\begin{itemize}
\item It is always easier to balance the workload if the granularity
  is {\em small} because it is always easier to distribute a set of
  objects evenly if we can {\em cut them into small pieces}.
\item However, there will be much more overhead not only in assigning
  these chunks to processors because the mapping table will be larger,
  but also in scheduling and synchronizing the processors because the
  number these operations will increase.  More details on the
  communication later.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Fine and Coarse}
\begin{itemize}
\item {\em Fine grain} parallelism partitions data/task into very
  small pieces, then assigns them to processors for processing.
\begin{itemize}
\item Suitable for system that can spawn a large number of threads
  with low cost, e.g., GPU.
\end{itemize}
\item {\em Coarse grain} parallelism partitions data/task into very
  large pieces, then assigns them to processors for processing.
\begin{itemize}
\item Suitable for system that can only spawn a limited number of
  threads, and the thread creation is expensive, e.g., CPU.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Give an example to illustrate the importance of granularity in
  partitioning workload.
\end{itemize}
\end{frame}

\section{Communication Optimization}

\begin{frame}
\frametitle{Communication Reduction}
\begin{itemize}
\item Communication is inevitable because multiple processors are
  working on the {\em same} problem.
\item Communication is overhead -- it does not appear in a sequential
  computation.
\item Communication should be reduced.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Principles}
\begin{itemize}
\item There are two basic principles to reduce communication.
\begin{itemize}
\item Low synchronization overheads
\item Data locality
\end{itemize}
\end{itemize}
\end{frame}

\subsection{Synchronization}

\begin{frame}
\frametitle{Synchronization}
\begin{itemize}
\item The synchronization is inevitable in parallel and distributed
  computing because we want to coordinate the processors.
\begin{itemize}
\item Barrier synchronization
\item Before/after synchronization
\item Access synchronization
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Barrier Synchronization}
\begin{itemize}
\item A computation may proceed in {\em stages} -- all processors
  needs to finish a stage before going to the next stage.
\begin{itemize}
\item This is usually called a {\em barrier} synchronization.  For
  example, all processors must combine their partial answer into the
  final answer.
\item This usually involves {\em all} processors.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Before/After Synchronization}
\begin{itemize}
\item In task parallelism one computation may need to precede another.
\begin{itemize}
\item You need to cook dinner before you can eat it.  This may be
  referred to as {\em before/after} synchronization.
\item This usually involves two processors -- one processor finishes a
  computation, then notifies the other processor to proceed.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Access Synchronization}
\begin{itemize}
\item Many processor may need to access a shared variable in a shared
  memory multiprocessor.
\begin{itemize}
\item Not an issue for distributed memory multicomputer since the
  computers do not share memory.
\end{itemize}
\item If the memory access is not synchronized properly, race
  condition may occur.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Synchronization Mechanism}
\begin{itemize}
\item Many parallel programming environments provide mechanism for
  program to specify synchronization {\em explicitly}.
\item The synchronization should be efficient.
\item The synchronization should be scalable, i.e., it should be
  efficient even if the number of processors involved is large.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Give an example for each synchronization described earlier.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Synchronization Mechanism}
\begin{itemize}
\item One can use message passing or shared memory to implement
  barrier synchronization within the same computer.
\item One can use signal inter-process communication to implement
  before/after synchronization within the same computer. 
\item One can use busy waiting or semaphore to implement the critical
  section for accessing shared variables.
\item If processor of different computers are involved in the
  synchronization, one needs to use network protocol to implement it.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Synchronization Optimization}
\begin{itemize}
\item The number of stages should be reduced.
\item The synchronization should be efficient.
\item The granularity should be carefully chosen to balance the
  overhead in synchronization and workload distribution.
\begin{itemize}
\item A fine-grain parallel computation is hard to synchronize, but
  easy to have even workload.
\item A coarse-grain parallel computation is easy to synchronize, but
  hard to have even workload.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Describe the inter-process communication (IPC) mechanism that
  you are aware of.
\end{itemize}
\end{frame}


\subsection{Data Locality}

\begin{frame}
\frametitle{Data Locality}
\begin{itemize}
\item {\em Locality} is a trend for a program to access
  data/instruction in {\em proximity}.
\item When a program access a data/instruction, it is very likely it
  will access the same data/instruction in the dear future, or it will
  access the data/instruction nearby in the near future.
\item Computer architecture explores locality for performance.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Temporal Locality} 
\begin{itemize}
\item When a program access a data/instruction, it is very likely it
  will access the same data/instruction in the dear future.
\item If we {\em cache} this data/instruction in a fast storage, then
  it is very likely we will be able to access the data fast.
\item Data/instruction are cached in data/instruction cache for
  performance.
\item CPU first tries to get the data from cache. If found then use
  it, otherwise the CPU gets the data from memory.
\item There could several levels of caches.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Performance}
\begin{itemize}
\item The performance comes from the difference in accessing speed to
  memory and cache, and the probability of being able to find the
  data/cache in cache.
\item If we can find the data/instruction in cache with high
  probability, i.e., with a high cache hit rate, then the performance
  will be improved.
\item If the temporal locality is good, which means the same
  data/instruction is likely to be used again in the near future, then
  we have good performance.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{In the Near Future?}  What do we mean ``in the near
future''?
\begin{itemize}
\item The capacity of cache is extremely limited.
\item When we access a data/cache, we have to place it into the cache
  for possible later references.
\item If the cache is {\em full}, then some data/instructions have to
  removed to make space for the incoming ones.
\item ``In the near future'' means when we want to access the
  data/instruction we placed into cache {\em again}, it is still
  there, i.e., before it was removed for making room for other
  data/instruction.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Other Applications}
\begin{itemize}
\item Hard disks maintain a small cache for data stored in the disk.
\item Operating system maintain disk cache for frequently accessed
  data on disk.
\item A translation lookaside buffer (TLB) is a cache for frequently
  accesses item in the page table.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Give an example of temporal locality.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Spacial Locality}
\begin{itemize}
\item When a program access a data/instruction, it will access the
  data/instruction {\em nearby} in the near future.
\item If we {\em cache} the near by data/instruction in a fast
  storage, then it is very likely we will be able to access the nearby
  data/instruction fast.
\item Parallel processing focuses on {\em spacial data locality}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Cache Line}
\begin{itemize}
\item Modern computer architecture does not cache data individually,
  instead it cache data/instruction in the unit of cache line.
\item A cache line consists of consecutive data/instruction in memory.
\item Nearby data/instructions are automatically cached for spacial
  locality.
\item Parallel programmers preserve data locality in a much higher
  ``data level'' when partitioning the data into chunks for processing.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Data Level Locality}
\begin{itemize}
\item When we assign data to processors for processing, we not only
  want to distribute them evenly, we also want to preserve {\em
    spacial data locality}.
\item That means when we want to process a data, the {\em required}
  data is {\em nearby}.
\begin{itemize}
\item What is required data?
\item What is ``near by''?
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Required Data}
\begin{itemize}
\item When we process a data, we usually need {\em other} data.
\item For example, when we want to compute vector $C$, which is the
  sum of two vectors $A$ and $B$.
\item We need $A_i$ and $B_i$ to compute $C_i$, then $A_i$ and $B_i$
  are required data of $C_i$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Owner}
\begin{itemize}
\item We usually follow a ``owner computes'' rule.
\item If a processor is the owner of a data, i.e., data is assigned to
  this processor, then it is responsible for the computation of this
  data.
\item The rule is simple and straightforward.
\item On rare occasion we will not follow the ``owner computes''
  rule.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Placement}
\begin{itemize}
\item If the length of the vector is 32, and we have two processors,
  how do we assign data to processors?
\item Intuitively we can place the first 16 elements of $A$, $B$, and
  $C$ to one processor, and the rest to the other processor.
\item The workload of computing $C$ is evenly distributed because each
  processor will compute 16 elements for $C$.
\item When a processor compute a $A_i$, it can get all the required
  data within its memory.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Wrong Placement}
\begin{itemize}
\item Again if the length of the vector is 32, and we have two
  processors.
\item We place the first 16 elements of $A$, $B$, and the last 16
  elements of $C$ to one processor, and the rest to the other
  processor.
\item The workload of computing $C$ is evenly distributed because each
  processor will compute 16 elements for $C$.
\item When a processor compute a $A_i$, it {\em cannot} get any
  required data within its memory.
\item Is this good?
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Nearby}
\begin{itemize}
\item ``Nearby'' means in the same processor.
\item We can access the required data within the processor of the same
  processor by {\em memory bandwidth}.
\item We can only access the required data within the processor of
  other processor by {\em network bandwidth}.
\item Memory bandwidth is {\em much much larger} than network
  bandwidth.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Local v.s. Remote}
\begin{itemize}
\item We use {\em Local memory} to indicate the memory of the same
  processor, and {\em remote memory} as the memory of other
  processors.
\item We conclude that {\em Local memory} is much much faster than
  {\em remote memory}.
\item This distinction applies only to distributed memory
  multicomputer.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Goal}
\begin{itemize}
\item If {\em most} of the required data is ``nearby'', then we have
  good performance.
\item That is, we want to make sure that most of the required data are
  nearby, i.e., in local memory, when we partition data to processor
  for computation.
\item Note that we say ``most'' because sometimes it is impossible to
  partition data so that all data access is local.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Give an example of spacial locality.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Matrix Multiplication}
\begin{itemize}
\item We multiple matrix $A$ and $B$ and get $C$.
\item The required data of $C_{ij}$ is the $i$'th row of $A$ and
  $j$'th column of $B$.  
\item If we insist that the required data must be in local memory,
  then everything will be in one processor!
\item This is against the principle of {\em even workload distribution}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Proof}
\begin{itemize}
\item $C_{ij}$ has to be in the same processor as the $i$'th row of
  $A$ and $j$'th column of $B$.
\item $C_{kl}$ has to be in the same processor as the $k$'th row of
  $A$ and $l$'th column of $B$.
\item Then $C_{kj}$ has to be in the same processor as the $k$'th row of
  $A$ and $j$'th column of $B$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Proof}
\begin{itemize}
\item This implies $C_{ij}$ and $C_{kj}$ have to be in the same
  processor, because they are in the same processor as the $j$'th column
  of $B$.
\item Similarly $C_{kj}$ and $C_{kl}$ have to be in the same
  processor, because they are in the same processor as the $k$'th row
  of $A$.
\item We conclude that $C_{ij}$ must be in the same processor as
  $C_{kl}$, for any $i$, $j$, $k$, and $l$.
\item Finally, all data will be in the same processor, which is bad.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Best Effort}
\begin{itemize}
\item If {\em most} of the required data is in local memory, then we
  have good performance.
\item We would like to increase the percentage of access local memory,
  which is a best effort.
\item The data has to be carefully partitioned to preserve locality.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Communication-to-Computation Ratio}
\begin{itemize}
\item Another way to understand the data locality is through the
  computation-to-communication ratio.
\item The amount of computation is roughly the same throughout
  different data partitioning.
\item The amount of communication is proportional to the amount of
  remote data, because local data do not incur communication.
\item If the communication-to-computation ratio is small then we have
  small communication overheads, which means we have good data
  locality.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Give an example of good locality and another example of bad
  locality for the same problem, due to different partitioning methods.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Surface to Volume Ratio}
\begin{itemize}
\item Sometimes we use a {\em surface-to-volume} ratio to explain
  communication-to-computation ratio.
\item We now consider the entire data as an object, and data
  partitioning is a way to cut the object into pieces.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Neighbors}
\begin{itemize}
\item In many computations the required data are those {\em
  neighboring} data.
\begin{itemize}
\item In an array the neighboring data for an array element are those
  that have indices differing from the element by 1.
\item In a graph the neighboring data are those node that are
  adjacent to the node.
\item In a graphic computation the neighboring data for a pixel are
  those that have adjacent to that pixel.
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Neighbors}
\begin{itemize}
\item In a table for dynamic programming the value of an element is
  usually determined by those elements that have indices differing
  from the element by 1.
\item In a page ranking algorithm the value of a node is determined by
  the neighboring nodes.
\item In a graphic relaxing problem the new value of a pixel is
  determined by the eight neighbors.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Give an example of computation that uses neighbors.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Pieces}
\begin{itemize}
\item We can use the ``volume'' of a piece to represent the number of
  data in a piece, which in turn represents the amount of {\em
    computation}.
\begin{itemize}
\item We assume that amount of workload is about the same for all
  data.
\end{itemize}
\item We can also use the ``surface area'' of a piece to represent the
  number of {\em required data} in a piece, which in turn represents
  the amount of {\em communication}.
\begin{itemize}
\item We assume that the required data are on the surface of the
  pieces.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Surface-to-volume Ratio}
\begin{itemize}
\item Now we can easily relate the computation-to-communication ratio
  to the surface-to-volume ratio.
\item We want to have small computation-to-communication ratio, then
  we must partition data into pieces that have small
  surface-volume-ratio.
\begin{itemize}
\item Surface area is communication.
\item Volume is computation.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Give an example of surface-to-volume ratio.  If an object has a
  large surface-to-volume ratio, is it easier, or harder, to coll
  down?  How does that relate to communication costs?
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{An Example}
\begin{itemize}
\item We are given a matrix of 32 by 32 by 32, and we would like to
  update each cell to be the average of its six neighbors with 8
  processors.
\item We have two choices.
\begin{itemize}
\item We cut the matrix into eight 16 by 16 by 16 cubes.
\item We cut the matrix into eight 4 by 32 by 32 slates.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Cubes}
\begin{itemize}
\item The volume of a cube is 16 by 16 by 16 = $4k$.
\item The surface area of a cube is $6 \times 16 \times 16 = 1.5k$.
\item The surface to volume ratio is $1.5/4 = 3/8$.
\item This means for the computation on each data the processors needs
  to access remote memory $3/8$ times.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Slates}
\begin{itemize}
\item The volume of a slate is 4 by 32 by 32 = $4k$.
\item The surface area of a slate is $2 \times 32 \times 32 + 4 \times
  32 \times 4 = 2.5k$.
\item The surface to volume ratio is $2.5/4 = 5/8$.
\item This means for the computation on each data the processors needs
  to access remote memory $5/8$ times, which is more than the $3/8$
  while cutting into cubes.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Lessons}
\begin{itemize}
\item Surface-to-volume ration is a reasonable estimate on the
  communication-to-computation ratio.
\item It is intuitive to partition the data into chunks so that the
  surface, i.e., communication, is minimized.  for example, if we
  partition the data into checker board pattern, the surface-to-volume
  ratio will be very large, and data locality will be poor.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Describe the difference in sizes of similar animals that live in tropical or Arctic area.
\end{itemize}
\end{frame}

\section{Efficient Implementation}

\begin{frame}
\frametitle{Efficiency}
\begin{itemize}
\item How to synchronize processors efficiently?
\begin{itemize}
\item Global synchronization
\item Point-to-point synchronization
\end{itemize}
\item How to transfer data efficiently?
\begin{itemize}
\item Batch mode message passing
\item Overlap communication with computation
\item Explore memory hierarchy
\end{itemize}
\end{itemize}
\end{frame}

\subsection{Synchronization}

\begin{frame}
\frametitle{Global Synchronization}
\begin{itemize}
\item Reduction
\begin{itemize}
\item Every processor has a value for the solution of its sub-problem,
  and we want to compute the {\em sum} of these values.
\item Every processor has a value for the solution of its sub-problem,
  and we want to compute the {\em minimum} of these values.
\item A reduction also serves as a barrier synchronization.
\end{itemize}
\item Barrier synchronization
\begin{itemize}
\item One can think of a barrier synchronization as a special form of
  reduction in which no value is exchanged.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Tree Optimization}
\begin{itemize}
\item We can ask a processor to coordinate the synchronization.
\begin{itemize}
\item Inherent sequential and the coordinator is the bottleneck.
\end{itemize}
\item Or we can organize the process as a tree. 
\begin{itemize}
\item We partition the processors into two subsets.
\item Two subsets recursively synchronize themselves {\em in
  parallel}.
\item Finally the two subsets synchronize with each other.
\item More details in lectures later.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Two Party Synchronization}
\begin{itemize}
\item In multiprocessor environment the critical section or semaphore
  may not be the best synchronization solution.
\item Unlike uni-processor environment, the overheads of critical
  section or semaphore is very high in multiprocessor environment.
\item Therefore we sometimes prefer spin-locks in multiprocessor
  environment, e.g., in Linux kernel data structure.
\end{itemize}
\end{frame}

\subsection{Data Transfer}

\begin{frame}
\frametitle{Transfer Efficiency}
\begin{itemize}
\item In many low level parallel programming environment,
  (e.g. OpenCL, CUDA, or MPI) the programmers can explicit control
  how data is transferred among professors.
\item In these environments the programmer can apply the following
  techniques to improve data transfer efficiency.
\begin{itemize}
\item Batch mode message sending
\item Overlap computation with communication
\item Explore memory hierarchy 
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Batch Mode}
\begin{itemize}
\item Many message passing system is built on top of network protocol
  like TCP/IP.
\item These protocol has a fixed start-up overhead, e.g., to establish
  a connection in TCP/IP.
\item If we send a large number of data through a connection, then the
  start-up overhead is amortized among the data begin transferred, which
  means we should transfer data in large quantity.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Overlap Communication with Computation}
\begin{itemize}
\item It is beneficial to have a large number of threads so that when
  a thread is waiting for data, other threads can use CPU resource for
  computation.
\item For example, in GPU the large number of running threads can hide
  memory latency, i.e., when a thread is waiting for memory other
  threads can use ALU for computations.
\item This requires a large number of threads, and a flexible
  scheduler to schedule them.  
\item This relieve the burden of cache.
\item More details in later lectures.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Explore Memory Hierarchy}
\begin{itemize}
\item In some parallel programming environment (e.g. CUDA and OpenCL),
  the programmer is free to move data with the memory hierarchy.
\item The processing units of GPU have fast and small local memory,
  and share a slow and large global memory.
\item CUDA and OpenCL programmers must {\em explicitly} move the data
  between the global and local memory to achieve performance.  This is
  tedious and error-prone process.
\item More details on later lectures.
\end{itemize}
\end{frame}

\end{CJK}
\end{document}

