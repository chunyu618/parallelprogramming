\documentclass{beamer}
\usetheme{Warsaw}

\usepackage{color}
\usepackage{CJK}
\usepackage{listings}
\usepackage{url}
\usepackage{booktabs}
\usepackage{pgf}
\usepackage{multicol}
\usepackage{algorithm}
\usepackage{algorithmic}

\input{../slide-macro}

\begin{document}
\begin{CJK}{UTF8}{bsmi}

\title{Parallel Algorithm Examples}

\author{Pangfeng Liu \\ National Taiwan University}

\begin{frame}
\titlepage
\end{frame}


\section{Embarrassingly Parallel}

\begin{frame}
\frametitle{Embarrassingly Parallel}
\begin{itemize}
\item An embarrassingly parallel computation is a collection of tasks
  that require {\em none} or {\em little} communication among them.
  In other words, they are {\em independent}.
\item This is ``embarrassing'' since nothing needs to be done to get
  good parallel performance.
\item People doing parallel processing, e.g. me, are not fond of this
  kind of computation.
\end{itemize}
\end{frame}

\subsection{Monte Carlo Method}

\begin{frame}
\frametitle{Monte Carlo Method}
\begin{itemize}
\item The Monte Carlo method repeats a {\em random process} to compute
  the answer.
\item We generate random numbers as input to the computation, so that
  the answer can be deduced from the random process.  Note that all
  computations are {\em independent}.
\item It is important to use different ``random seed'' with these
  tasks, so that the results from them are {\em statistically
    independent}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Compute $\pi$}
\begin{itemize}
\item Randomly throw darts into a square with a inscribed circle.
\item Compute the number of darts that fall into and outside the
  circle.
\item Compute the probability that a darts falls into the circle.
\item Multiple the probability by 4 to approximate $\pi$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Notes}
\begin{itemize}
\item This is not a good example because people will not compute $\pi$
  this way.
\item Nevertheless this is an easy-to-understand example to illustrate
  the independence of tasks.
\end{itemize}
\end{frame}

\subsection{Parameter Search}

\begin{frame}
\frametitle{Parameter Search}
\begin{itemize}
\item Suppose we want to fins the a set of ``best'' parameters $x =
  (x_1, \ldots, x_n)$, which maximize an objective function $y =
  f(x)$.
\item We also assume that the $f$ function is very complex so that we
  cannot deduce the values of $f(x)$'s for $x$'s that we have not yet
  computed the function values, from those function values that we
  have already computed.
\item It is easy to dispatch the computation of $f(x)$ to processors to
  speed up the parameter search, since there is no dependency between
  these computations.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{File Serving}
\begin{itemize}
\item A web server serves static HTML files to clients.
\item The requests of clients are independent, so the web server
  simply serves the files in parallel, maybe using multiple threads.
\item If there are multiple web servers, they can also serve the files
  in parallel.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Summary}
\begin{itemize}
\item Embarrassingly parallel computation has good speedup and
  efficiency, because the tasks are independent and do not require
  significant communication.
\item It is trivial to dispatch tasks to processors if the tasks
  require roughly the same amount of time.
\item It is non-trivial to dispatch tasks to processors if the tasks
  require very different amount of time.  In this case ``load
  balancing'' is required.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Give an example of embarrassingly parallel computation.
\end{itemize}
\end{frame}

\section{Divide-and-Conquer Examples}

\begin{frame}
\frametitle{Divide and Conquer}
\begin{itemize}
\item Divide-and-conquer is a common parallel algorithm design
  technique.
\item As in a sequential divide-and-conquer algorithm, the problem is
  first divided into sub-problems.
\item Unlike a sequential divide-and-conquer algorithm, a parallel
  algorithm solves (conquer) the sub-problem {\em in parallel}.
\item Some communication may be necessary since the sub-problems may
  have dependency on each other.
\item Finally the answers from individual sub-problems are combined
  into the final answer.
\end{itemize}
\end{frame}


\subsection{Summation}

\begin{frame}
\frametitle{Summation}
\begin{itemize}
\item We want to sum $n$ numbers, and $n$ is very large.
\item It is easy to see that we can apply divide-and-conquer technique
  to solve the problem in parallel with $p$ processors.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Parallel Summation Algorithm}
\begin{enumerate}
\item Partition the numbers so that each processor has roughly $n/p$
  numbers.
\item Each processor computes the sum of assigned numbers.
\item A processor collects all the partial sums from other processors
  and compute the final sum.
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Analysis}
\begin{itemize}
\item We assume that first step takes very little time.\footnote{This
  is a the case for shared memory model, but not necessarily true for
  distributed memory model.}
\item The second steps takes $O({n \over p})$ times.
\item The third steps takes $O(p)$ times.
\end{itemize}
The time complexity is as follows.
\begin{equation}
O({n \over p} + p)
\end{equation}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item How do we minimize the $O({n \over p} + p)$ by choosing the
  right $p$?
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Communication}
\begin{itemize}
\item Having one processor collect all the answer is not efficient.
\item We partition the processor into two groups.  Every processor in
  the first group sends its answer to the corresponding processors in
  the second group.
\item We repeatedly do this until we have only one processor left,
  who should have the final answer.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Parallel Summation Algorithm}
\begin{enumerate}
\item Partition the numbers so each processor has $n/p$ numbers.
\item Each processor computes the sum of its numbers.
\item Use the recursive algorithm to compute the final sum.
\item This is similar to the ``tree optimization'' in synchronization.
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Analysis}
\begin{itemize}
\item We assume that first step takes very little time.
\item The second steps takes $O({n \over p})$ time.
\item The third steps takes $O(\log p)$ time because the depth of a
  complete binary tree of $n$ nodes is about $O(\log n)$.
\end{itemize}
The final time complexity is as follows.
\begin{equation}
O({n \over p} + \log p)
\end{equation}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Describe the difference between the previous two algorithms.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Observation}
\begin{itemize}
\item The first term $({n \over p})$ is {\em computation}.  We can
  {\em never} reduce this part.
\item The second term $(\log p)$ is about {\em communication}.  We try
  our best to reduce this part.
\item If we increase $p$, the computation time decreases and the
  communication time increases.  That means we have more workers to
  share the workload, but we need to communicate more among more
  workers.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{More Observations}
\begin{itemize}
\item It is important to balance the load among processors, i.e. we
  want to send $({n \over p})$ data to each processor for processing.
\item A shared memory implementation is significantly easier than a
  distributed memory implementation.
\item The recursive (or tree like) communication pattern is much more
  complicated than a naive one, and requires much more complicated
  synchronization.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Analysis}
Speedup
\begin{equation}
k = {n \over {{n \over p} + \log p}}
\end{equation}
Efficiency
\begin{equation}
e = {n \over {{n + p \log p}}}
\end{equation}
\end{frame}

\begin{frame}
\frametitle{Choice of $p$}
\begin{itemize}
\item What is the best $p$ in terms of speedup?
\item Set ${n \over p} = \log p$ and solve $p = {n \over {\log n}}$.
\item The minimum parallel execution time $\Theta(\log n)$ is achieved
  when $p = \Theta({n \over {\log n}})$.
\item If we set $p = \sqrt n$, then the time will be $\Theta(\sqrt
  n)$, which is much larger than the optimal $\Theta(\log n)$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Use calculus to compute the optimal $P$ value.
\item What will happen if we set $p$ to $n$?
\item Is this ``theoretical'' optimal $p$ useful in practice?
\end{itemize}
\end{frame}

\subsection{Parallel Prefix}

\begin{frame}
\frametitle{Prefix Sum} Given $n$ numbers $(x_1, \ldots, x_n)$, we
want to compute all prefix sums as follows.
\begin{equation}
s_k = \sum_{i = 1}^k x_i
\end{equation}
\end{frame}

\begin{frame}
\frametitle{Compact An Array}
\begin{itemize}
\item The prefix sum has various applications.
\item If there are zeros and non-zeros in an array $A$ and we only
  wish to keep the non-zeros in a new array $B$, then we can do a
  prefix sum on another array $P$ with 0 and 1 to determine the
  positions of non-zeros in $B$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Pact an Array}
\centerline{\pgfimage[height=0.7\textheight]{pact.pdf}}
\end{frame}

\begin{frame}
\begin{itemize}
\frametitle{Fibonacci's Numbers}
\item The prefix sum has various applications, and it is not limited
  to summation.
\item We all know Fibonacci's numbers. 
\begin{equation}   
f_i = \left\{
\begin{array}{ll}
0 &  i = 0 \\  
1 &  i = 1 \\  
f_{i - 1} + f_{i - 2} &  i \geq 2
\end{array}
\right.
\end{equation}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Fibonacci's Numbers}
\begin{eqnarray}
f_1 & = & f_1 \\
f_2 & = & f_0 + f_1 \\
\left( 
\begin{array}{c}
f_1 \\ f_2 
\end{array}
\right) 
& = & 
\left( 
\begin{array}{cc}
0 & 1 \\ 1 & 1 
\end{array}
\right) 
\left( 
\begin{array}{c}
f_0 \\ f_1 
\end{array}
\right)  \\
\left( 
\begin{array}{c}
f_2 \\ f_3 
\end{array}
\right) 
& = & 
\left( 
\begin{array}{cc}
0 & 1 \\ 1 & 1 
\end{array}
\right) 
\left( 
\begin{array}{c}
f_1 \\ f_2 
\end{array}
\right) \\
& = & 
\left( 
\begin{array}{cc}
0 & 1 \\ 1 & 1 
\end{array}
\right) 
\left( 
\begin{array}{cc}
0 & 1 \\ 1 & 1 
\end{array}
\right) 
\left( 
\begin{array}{c}
f_0 \\ f_1 
\end{array}
\right) \\
\left( 
\begin{array}{c}
f_i \\ f_{i+1} 
\end{array}
\right) 
& = & 
\left( 
\begin{array}{cc}
0 & 1 \\ 1 & 1 
\end{array}
\right) ^ i
\left( 
\begin{array}{c}
f_0 \\ f_1 
\end{array}
\right)
\end{eqnarray}
\end{frame}

\begin{frame}
\frametitle{Prefix Product} Given $n$ matrices $(x_1, \ldots, x_n)$, we
want to compute all prefix products as follows.
\begin{equation}
s_k = \Pi_{i = 1}^k x_i
\end{equation}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Give an example of prefix sum application.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Parallel Prefix Sum Algorithm}
\begin{itemize}
\item Use the $k$-th processor to compute $s_k$.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Analysis}
\begin{itemize}
\item A sequential algorithm can do this easily in $O(n)$ time.
\item We assume that we use one processor per data, so $p = n$.
\item The $k$-th processor requires $O(k)$ time.
\item The parallel time is the maximum of all processor time, hence
  $O(n)$.
\item The speedup is $O(1)$, and the efficiency is $({1 \over p})$. 
\item Not very efficient.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item What is wrong with the previous algorithm?
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Parallel Prefix Sum Algorithm}
\begin{itemize}
\item To avoid doing duplicated work, we again use the $k$-th processor
  to compute $s_k$, but we get the result from the $k-1$-th processor.
\begin{equation}
s_k = s_{k-1} + x_k
\end{equation}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Analysis}
\begin{itemize}
\item Now we do not duplicate work, but we need to {\em wait}.
\item The $k$-th processor cannot compute its sum before receiving
 $s_{k-1}$.
\item The result will go ripple-like from the first to the last
  processor like a wave-front.
\item The parallel time is the maximum of all processor time, hence
  $O(n)$.
\item The speedup is $O(1)$, and the efficiency is $({1 \over n})$. 
\item Again not very efficient.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item What is wrong with the previous algorithm?
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{A Better Algorithm}
\begin{itemize}
\item There are $\log n$ stages.
\item In the $i$-stage every element adds the element $2^i$ to the
  left to itself.
\begin{itemize}
  \item In the first stage every element adds the element to its
    left to itself.
  \item In the second stage every element adds the element two
    elements to its left to itself.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{A Better Algorithm}
  \frametitle{Parallel Prefix Sum Algorithm}
  \begin{algorithmic}
    \FOR {$i \leftarrow$ 0 TO $\log{n} - 1$}
    \FOR {$k \leftarrow 2^i$ TO $n$} 
    \STATE x[k] += x[k - $2^i$] \COMMENT{The $k$-th processor receives a partial sum from the $k - i$-th processor.}
    \ENDFOR 
    \ENDFOR
  \end{algorithmic}
\end{frame}

\begin{frame}
\frametitle{Parallel Prefix}
\centerline{\pgfimage[height=0.7\textheight]{prefix.pdf}}
\end{frame}

\begin{frame}
\frametitle{Analysis}
\begin{itemize}
\item There are $\log n$ steps, since $p = n$ now.
\item A processor does a sum and receives a message, so the time is
 $O(1)$.
\item The total parallel time is $O(log n)$, which is much better than
  $O(n)$ in previous approaches.
\item The speedup is $O({n \over log n})$, and the efficiency is $({1
  \over {\log n}})$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Prove that the algorithm is correct.
\item What is the possible problem with the previous algorithm?
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Improvement}
\begin{itemize}
\item It is not practical to use one processor per data, since in
  practice the number of data is much more than the number of
  processors.
\item We will assume that $n$ is much larger than $p$, hence we need
  to partition the data among processors.
\item Now each processor will compute the prefix sum of its data
  first, then use the previous algorithm to ``patch'' things up.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Improved Algorithm}
\begin{enumerate}
\item Partition data among processors.
\item Each processor computes its prefix sum.
\item Use the previous algorithm to compute the prefix sum of the {\em
  last} elements from all processors.
\item Use the prefix sum from the last elements to patch up the answers.
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Parallel Prefix}
\centerline{\pgfimage[height=0.7\textheight]{segment.pdf}}
\end{frame}

\begin{frame}
\frametitle{Analysis}
\begin{itemize}
\item A sequential algorithm can do this easily in $O(n)$ time.
\item The first step does not take time.
\item Both the second and the fourth step take $O({n \over p})$ time.
\item The third step takes $O(\log p)$, as discussed before.
\begin{equation}
T_p = O({n \over p} + \log p)
\end{equation}
\item Similar optimization can find good $p$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Final Notes}
\begin{itemize}
\item What did we compute?
\begin{eqnarray}
s_4 & = & (((x_1 + x_2) + x_3) + x_4) \\
s_4 & = & ((x_1 + x_2) + (x_3 + x_4))
\end{eqnarray}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item What property the operation must have in order for this
  algorithm to work?
\item Does ``+'' have this property?
\item Does ``maximum'' have this property?
\item Does matrix multiplication have this property?
\end{itemize}
\end{frame}

\subsection{Parallel Sorting}

\begin{frame}
\frametitle{Sorting}
\begin{itemize}
\item To sort keys between $1$ and $n$ in order.
\item We try the bucket sort first.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Bucket Sort}
\begin{itemize}
\item We need $b$ buckets.
\item We need to know the range of keys, and we assume that the
  keys are {\em evenly} distributed in this range.
\item We use an array element to record whether a key {\em appears}
  in the input.
\item We do not compare!
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Bucket Sort}
\begin{enumerate}
\item Scan the keys and record its appearance in the corresponding
  buckets.
\item Read the keys from the buckets.
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Bucket Sort}
\programlisting{bucket}{Bucket sort}
\footnote{\url{http://www.eecs.ucf.edu/courses/cop3502h/spr2007/sorting3.pdf}}
\end{frame}

\begin{frame}
\frametitle{Bucket Sort}
\centerline{\pgfimage[height=0.6\textheight]{bucket.pdf}}
\end{frame}

\begin{frame}
\frametitle{Analysis}
\begin{itemize}
\item The first step takes $O(n)$ time.
\item The second step takes $O(n + b)$ time.
\item The total complexity is $O(n + b)$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Why the second step takes $O(n + b)$, not $O(n b)$ time?
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Parallel Bucket Sort}
\begin{enumerate}
\item Every processor has exactly one bucket.
\item Every processor scans all keys to record keys corresponding to
  its bucket.
\item Read the keys from the buckets.
\end{enumerate}
\end{frame}


\begin{frame}
\frametitle{Bucket Sort}
\centerline{\pgfimage[height=0.6\textheight]{parallelbucket.pdf}}
\end{frame}

\begin{frame}
\frametitle{Parallel Bucket Sort}
\begin{itemize}
\item Every processor takes $O(n)$ time just to scan data.
\item Exactly how to ``read the keys from the bucket''?
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item What is wrong with the previous algorithm?
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Parallel Bucket Sort}
\begin{itemize}
\item Every processor reads only $n \over p$ keys, and record the
  appearance into its own set of buckets.
\item Now it only takes $O({n \over p})$ time.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Read Keys}
How do we read the keys out in the second step?
\begin{enumerate}
\item Every processor remembers the number of keys it places in every
  bucket.
\item Each processor computes the number of keys that should be in its
  bucket.
\item Use the parallel prefix sum algorithm to know the starting
  position each bucket should start.
\item Actually read from the bucket.
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Bucket Sort}
\centerline{\pgfimage[height=0.8\textheight]{parallelbucket-2.pdf}}
\end{frame}

\begin{frame}
\frametitle{Analysis}
\begin{enumerate}
\item The first step takes $O({n \over p})$ time.
\item The second step takes $O(b)$ because each processor needs to add
  $b$ numbers.
\item The third step takes  $O(\log p)$.
\item the fourth step takes $O({n \over p})$.\footnote{Assuming the
  keys are evenly distributed among processors.}
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Final Time Complexity}
\begin{enumerate}
\item The ``read to bucket'' takes $O({n \over p})$ time.
\item The ``read from bucket'' takes $O({n \over p} + \log p + b)$ time.
\item The total time complexity is $O({n \over p} + \log p + b)$.
\item The speedup is $O({{n + b} \over {{n \over p} + \log p + b}})$, which is $O({{n + b} \over {\log n + b}})$ when we set $p$ to $n \over {\log n}$.
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item What is wrong with the previous algorithm?
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Improvement}
\begin{itemize}
\item The bucket sort uses (and wastes) a lot of memory.
\item We will try a ``quicker'' sort that also uses the
  divide-and-conquer techniques.
\item Again we assume that the keys are evenly distributed and we know
  the range.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Sequential ``Quicker'' Sort}
Sort the keys recursively as follows.
\begin{itemize}
\item Partition keys into $g$ groups according to $g-1$ pivots.
\item Individually sort the keys in each group recursively.
\item Concatenate all the keys from the $g$ groups.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Analysis}
\begin{itemize}
\item The first step takes $O(n)$ time.
\item Let the time to sort $n$ keys be $T(n)$.
\begin{equation}
T(n) = g T({n \over g}) + n
\end{equation}
\item It is easy to see that $T(n) = O(n \log_{g} n) = O(n \log n)$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Explain what will happen when there are only two groups in the
  ``quicker'' sort.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Parallel Quicker Sort}
Every processor manages a group, which will store a range of keys.
\begin{enumerate}
\item Every processor reads only $n \over p$ keys, and puts them into
  the corresponding bucket.
\item Each processor sorts the keys in its bucket.
\item Read all keys from processors.
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Pact an Array}
\centerline{\pgfimage[height=0.7\textheight]{parallelquicker.pdf}}
\end{frame}

\begin{frame}
\frametitle{Analysis}
\begin{itemize}
\item The first step takes $O({n \over p})$ time, without considering
  the synchronization.
\item The second step takes $O({n \over p} \log {n \over p})$,
  assuming that the keys are evenly distributed.
\item The third step takes $O(\log p)$ from previous analysis
  on prefix sum.
\item The total time is $O({n \over p} \log {n \over p} + \log p)$.
\item The speedup is $O({{n \log n} \over {{{n \over p} \log {n \over
      p} + \log p}}})$.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item What synchronization mechanism do we need for the first step in
  the previous algorithm?  
\item Why we did not need it in the previous parallel bucket sort?
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Exchange Sort}

\begin{itemize}
\item We consider a recursive ``exchange'' sort that is much more
  suitable for distributed memory multicomputers.
\item This algorithm is very suitable for hypercube.
\item We do not require that the keys are distributed uniformly, since
  we can argue that the performance is {\em statistically} acceptable.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Exchange Sort}
\begin{itemize}
\item Divide the processors into two groups of equal size.
\item Each processor ``exchanges'' keys with its corresponding
  processor in the other group according to a pivot -- smaller keys go
  to a group of processors and bigger keys go to the other group.
\item Recursively do the same for both groups.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Exchange Sort}
\begin{enumerate}
\item Find a pivot.
\item Divide the processors into two groups of equal size.
\item Each processor ``exchanges'' keys with its corresponding
  processor in the other group.
\item Recursively do the same for both groups.  
\item Finally each processor sorts its keys.
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Pivot}
\begin{itemize}
\item How to find a pivot?
\item This is like the argument sequential quick sort, we just
  randomly pick one, which is good enough.
\item Use a binomial trial to argue that the tree depth of a quick
  sort is bounded by $O(\log p)$ with high probability.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Analysis}
\begin{itemize}
\item We focus on the number of ``movements'' as the cost, since
  basically no computation is involved.
\item In each level of the exchange a processor exchanges at most ${n
  \over p}$ keys.
\item From previous argument the depth of the tree is bounded by
  $O(\log p)$, so the cost of exchange is $O( {n \over p} \log p)$.
\item Finally each processor still needs to sort its key with $O({n
  \over p} \log {n \over p})$ time.
\item The final complexity is $O({n \over p} (\log p + \log {n \over
  p})) = O({n \over p} \log n)$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item What is the theoretical speedup of this algorithm?
\item Find out the definition of hypercube and why is this algorithm
  suitable for hypercube.
\end{itemize}
\end{frame}


\subsection{Matrix Multiplication}

\begin{frame}
\frametitle{Matrix Multiplication}
\begin{itemize}
\item Multiple two $n \times n$ matrices $A$, and $B$ and place the
  result into $C$.
\begin{equation}
A \times B = C
\end{equation}
\item We assume that the matrix is dense.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Sequential Matrix Multiplication}
\begin{itemize}
\item For the interest of simplicity we use the standard $O(n^3)$
  algorithm, instead of the
  Stassen\footnote{\url{http://en.wikipedia.org/wiki/Strassen_algorithm}}
  algorithm.
\item The time complexity is $O(n^3)$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Parallel Matrix Multiplication}
\begin{itemize}
\item We use $p$ processors to compute the $n^3$ elements in $C$.
\item Each processor simply computes the answer and no communication
  among them is necessary for a shared memory implementation. 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Analysis}
\begin{itemize}
\item Each processor computes ${n^2 \over p}$ elements. 
\item Each elements takes $O(n)$ time to computes.
\item The parallel time is $O({n^3 \over p})$.
\item The speedup is ${n^3 \over {n^3 \over p}} = p$.  It seems to an
  embarrassingly parallel computation and nothing can be improved.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Why no communication among processors is necessary for a shared
  memory implementation of the previous algorithm during the
  computation stage?
\end{itemize}
\end{frame}


\end{CJK}
\end{document}

