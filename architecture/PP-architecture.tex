\documentclass{beamer}
\usetheme{Warsaw}

\usepackage{color}
\usepackage{CJK}
\usepackage{listings}
\usepackage{url}
\usepackage{booktabs}
\usepackage{pgf}
\usepackage{multicol}

\input{../slide-macro}

\begin{document}
\begin{CJK}{UTF8}{bsmi}

\title{Parallel Computer Architecture}

\author{Pangfeng Liu \\ National Taiwan University}

\begin{frame}
\titlepage
\end{frame}

% \section{Introduction} 

\begin{frame}
\frametitle{Architecture}
\begin{itemize}
\item Multiprocessor
\item Multicomputer
\item Flynn's Taxonomy
\end{itemize}
\end{frame}

\section{Multiprocessor}

\begin{frame}
\frametitle{Uniprocessor}
\centerline{\pgfimage[height=0.4\textheight]{single-processor.pdf}}
\begin{itemize}
\item This is how we think of a computer.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Uniprocessor}
\begin{itemize}
\item It has a single processor for computation.
\item It has a single memory for storing instructions and data.
\item The CPU fetches instructions from memory, executes the instructions, updates the contents of registers and possibly data in the memory, and then repeats.
\item Intel, AMD, ARM, etc.
\end{itemize}
\end{frame}


\subsection{Multiple Uniprocessors}

\begin{frame}
\frametitle{Multiple Uniprocessors}
\begin{itemize}
\item It is intuitive to have multiple uniprocessors working together to have high performance.
\item We believe that we can solve the problem faster/better if more processors work together.
\item How do they work {\em together}?
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Work Together?}
\centerline{\pgfimage[width=\textwidth]{multiple-single-processor}}
\end{frame}

\begin{frame}
\frametitle{Communication} 
\huge Processors must communicate to
coordinate their actions, and exchange data if necessary.
\end{frame}


\begin{frame}
\frametitle{Work Together?}
\centerline{\pgfimage[width=\textwidth]{multiprocessor-single-memory}}
\begin{itemize}
\item We cannot simply connect all processors to a memory.
\item The memory will be {\em inconsistent}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Race Condition}
\begin{itemize}
\item Suppose two processors want to add 1 to the same counter.
\item The first processor fetches the old contents of the counter and adds 1 to it.
\item Before the first processor can store the new content, and the second processor fetches the old content.
\item The first processor now stores its new content to memory.
\item The second processor adds 1 to the old content and stores the new content back to memory.
\item The counter only increases by 1, which is {\em incorrect}.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Multiple Uniprocessors}
\begin{itemize}
\item We put a {\em memory management/arbitration unit} between the
  processors and the memory so that every processor can access memory
  in a consistent manner.
\item This memory management unit must be very efficient in providing
  {\em point-to-point} data transfer so as to provide fast memory
  access for every processor.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Multiprocessor}
\centerline{\pgfimage[width=\textwidth]{multiple-processor-switch}}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item What could happen if three processors want to add 1, 2, and 4
  into a memory wiht an initialized value 0?
\end{itemize}
\end{frame}

\subsection{Shared Memory}

\begin{frame}
\frametitle{Shared Memory}
\begin{itemize}
\item Logically we do not care about the memory management unit -- we
  simple believe that every processor can access this {\em shared
    memory}.
\item This shared memory provides a {\em shared addressing space} for
  all processors.
\item This {\em shared} memory is also a {\em global memory}.
\item The cost for every processor to access every part of the memory is the same, and then we have Uniform Memory Access (UMA).
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Distributed Shared Memory}
\begin{itemize}
\item It is a distributed version of shared memory.
\item Every processor has local memory, and the collection of the local memory forms a global memory.
\item We connect the processors to a memory management unit, which
  determines the address is local or remote.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Multiprocessor}
\centerline{\pgfimage[width=\textwidth]{distributed-multiprocessor}}
\end{frame}

\begin{frame}
\frametitle{Distributed Multiprocessor}
\begin{itemize}
\item If the address is local, then it is retrieved from the
  local memory.
\item If the address is remote, it is retrieved from someone else's local memory.
\item The cost for every processor to access every part of the memory
  is {\em not} the same, then we have Non-Uniform Memory Access
  (NUMA).
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Describe the difference between NUMA and UMA.
\item Recall that each A64FX processor has four NUMA nodes.  What does that mean?
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Multiprocessor}
\begin{itemize}
\item Logically we do not care about the memory management unit -- we
  simple believe that every processor can access this {\em shared
    memory}.
\item This shared memory provides a {\em shared addressing space} for
  all processors.
\item This {\em shared} memory is also a {\em global memory}.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{A Logical View}
\begin{itemize}
\item In summary, we can think of our multiprocessor is like this.
\end{itemize}
\centerline{\pgfimage[width=\textwidth]{multiprocessor}}
\end{frame}

\begin{frame}
\frametitle{Multiprocessor}
\begin{itemize}
\item Conceptually multiple processors are connected by a shared
  memory -- that is all we need to know in terms of programming.
\item For some programming models, we still need to consider the possibility of race condition and use the construct provided by the programming model to avoid it, e.g., Pthread.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Functions} 
\begin{itemize}
\item Every processor has its computing resource, like ALU, registers, etc., so you can run multiple processes on them simultaneously.
\item Every processor works on its assigned tasks using its computing resource.
\item Every processor can read and write the shared memory to communicate or synchronize with other processors.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Synchronization}
\begin{itemize}
\item Processor can synchronize with each other by shared memory.
\item For example, in a barrier synchronization, a processor cannot proceed until all others have reached the same barrier.
\item We can set a shared variable {\tt count}. 
Every finished processor adds 1 to {\tt count}. 
When the value of {\tt count} reaches the number of processors, then every processor knows that it has synchronized with everyone.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Describe the dependency graph using {\em functions} and {\em
  synchronization}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Memory Conflict}
\begin{itemize}
\item Multiple processors can access the memory at the same time,
  causing conflicts.
\item When a computation has different outcome due to different
  execution order, we have a {\em race condition}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Software Solution}
\begin{itemize}
\item Programming environment must provide mechanism that prevents
  race condition.
\item Critical section, lock, synchronization, etc.
\item More specific details will be provided when we discuss parallel
  programming.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Describe the concept ``critical section''.
\end{itemize}
\end{frame}


\subsection{Cache}

\begin{frame}
\frametitle{Cache}
\begin{itemize}
\item To make the case even worst, we need to deal with the cache.
\item Cache is fast memory, usually small and expensive.
\item To improve performance, we put often-used data/instructions in a  cache. 
If we need the data/instructions again, we can access them in the fast cache instead of the slow main memory.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Cache for Uniprocessor}
\centerline{\pgfimage[height=0.6\textheight]{single-processor-cache}}
\end{frame}

\begin{frame}
\frametitle{Race Condition}
\begin{itemize}
\item Now every processor has a cache, and it can take data from its
  cache, not the main memory.
\item Suppose one processor changes the content of the memory, what
  should happen to the cached data in some processor's cache?
\item If one processor changes the data in its cache, would other
  processor be able to notice this change?
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Cache}
\centerline{\pgfimage[width=\textwidth]{multiprocessor-cache}}
\end{frame}

\begin{frame}
\frametitle{Hardware Solution}
\begin{itemize}
\item The hardware must guarantee that the memory and cache are in a consistent state. There are various levels of guarantees.
\item If one processor changes the content of the memory, the hardware should invalidate data that other processors have cached.
\item If one processor changes the data in its cache, other processors should be able to see it.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Writing Policy}
\begin{description}
\item[Write-through] Write is done synchronously both to the cache and
  to the backing store.
\item[Write-back] Initially, writing is done only to the cache. The
  write to the backing store is postponed until the cache blocks
  containing the data are about to be accessed by others.
\end{description}
\end{frame}


\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Describe the difference between write-through and write-back caching.
\item Describe the advantages of write-through and write-back caching.
\end{itemize}
\end{frame}


\subsection{Examples}

\begin{frame}
\frametitle{ARM/Fujitsu A64FX CPU} 
\begin{itemize}
\item The A64FX is the world's first processor to implement Scalable
  Vector Extension (SVE), an extension of the Armv8.2-A instruction
  set architecture for supercomputers.
\item It has 48 calculation cores and two or four assistant cores, and
  has a theoretical peak performance of 3.3792 teraflops in double
  precision floating-point calculations.
\item It is the building block of supercomputer Fugaku. 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{A64FX}
\centerline{\pgfimage[width=0.9\textheight]{fujitsu-a64fx.jpg}}
\footnote{\url{https://www.nextplatform.com/2021/06/22/u-s-institutions-put-fujitsu-a64fx-through-the-paces/}}
\end{frame}

\begin{frame}
\frametitle{A64FX Specifications}
\url{https://www.fujitsu.com/downloads/SUPER/a64fx/a64fx_datasheet_en.pdf}
\end{frame}

\begin{frame}
\frametitle{L2 Cache}
\begin{itemize}
\item The memory controller controls and coordinates the access to shared memory.
\item Cores can communicate with the queue.  
\item Three cores share four L2 caches.  
  \begin{itemize}
    \item Shared L2 cache indicates that if we place wrong
      processes/threads into the cores that share the L2 cache, the
      performance will suffer.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discusson}
\begin{itemize}
\item Can you tell from the A64FX picture which cores share which L2 cache?
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Nvidia Tesla}
\begin{itemize}
\item The Tesla graphics processing unit (GPU) is Nvidia's third brand
  of GPUs. 
\item Tesla is based on high-end GPUs from the G80 (and on), as well
  as the Quadro lineup.
\item Tesla is Nvidia's first dedicated {\em General Purpose GPU}
  (GPGPU).
\item \url{http://en.wikipedia.org/wiki/Nvidia_Tesla}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{GPU Architecture}
\centerline{\pgfimage[height=0.8\textheight]{GPU-hardware}}
\end{frame}

\begin{frame}
\frametitle{Nvidia Tesla}
\centerline{\pgfimage[width=0.8\textwidth]{tesla}}
\end{frame}

\begin{frame}
\frametitle{C1060}
\begin{itemize}
\item 240 processors at 1.30 GHz.
\item 4096 MB of GDDR3 memory.
\item 102.4 GB/s memory bandwidth.
\item 933.12 GFLOPs single precision, 77.76 GFLOPs double precision.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Tesla Architecture}
\centerline{\pgfimage[width=0.8\textwidth]{tesla-architecture}}
\end{frame}

\begin{frame}
\frametitle{Characteristics}
\begin{itemize}
\item A GPU is a device that needs a host to provide data and instructions.
\item A GPU has many cores -- usually much more than a CPU.
\item CPU cores receive instructions from the host for execution, which means they must run the same set of instructions.
\item  GPU cores have their local memory and share a device memory.
\item More details in the ``OpenCL'' lectures.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Describe the difference between CPU and GPU.
\end{itemize}
\end{frame}


\section{Multicomputer}

\begin{frame}
\frametitle{Multiple Uniprocessor}
\begin{itemize}
\item It is intuitive to have multiple uniprocessors working together
  to have high performance.
\item How do they work {\em together}?
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Work Together}
\centerline{\pgfimage[width=\textwidth]{multiple-single-processor}}
\end{frame}


\begin{frame}
\frametitle{Multiple Uniprocessor}
\begin{itemize}
\item We connect all the computers with a {\em network}, so they can
  send {\em messages} to each other.
\item This network could be very slow or fast, depending on the
  applications.
\item The network must provide {\em point-to-point} data transfer to
  move data from one processor to another.
\item Processors can only synchronize themselves via the network, which
  is a difficult task.
\end{itemize}
\end{frame}

\subsection{Network}

\begin{frame}
\frametitle{A Network}
\centerline{\pgfimage[width=\textwidth]{multicomputer}}
\end{frame}

\begin{frame}
\frametitle{Functions} 
\begin{itemize}
\item Every processor works on the tasks assigned to it, using its own
  computing resource.
\item Every processor can send message to each other, so as to
  communicate or synchronize with other processors.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Multicomputer}
\begin{itemize}
\item Sometimes we use the term {\em node}, since these processors are
  by themselves {\em computers}.
  \begin{itemize}
    \item Remember the 16,000 nodes in the Tianhe 2 cluster.
  \end{itemize}
\item Each node has its own CPU's, memory, even I/O devices.
\item The nodes can communicate with each other by the network, usually
  through standard TCP/IP protocol.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Memory}
\begin{itemize}
\item There is no {\em shared addressing space} for all processors --
  each processor use its own memory.
\item The memory of every processor is called {\em local memory}.
\item Since a memory is accessed by only one processor, we do {\em
  not} have memory conflict.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Synchronization}
\begin{itemize}
\item Processor can synchronize with each other by messages.
\item In a barrier synchronization, a processor cannot proceed until
  all others have reached the same conclusion.
\item Every processor sends a message to a master after finishing its work.
\item When the master receives a message from every processor, he knows everybody has finished.
\item The master sends a message to everyone that it can continue.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Compare the way multicomputer and multiprocessor does a barrier
 synchronization.
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Network}
\begin{itemize}
\item We do care about the network -- a fast network provide better
  connectivity.\footnote{\url{http://en.wikipedia.org/wiki/Network_bandwidth}}
\begin{itemize}
\item Ethernet 10Mbit/s
\item Fast Ethernet 100Mbit/s
\item Gigabit Ethernet 1 Gbit/s
\item 10 Gigabit Ethernet 10  Gbit/s
\item Myrinet 10 Gbit/s
\item 100 Gigabit Ethernet 100 Gbit/s
\item InfiniBand (12X EDR) 300 Gbit/s
\end{itemize}

\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Topology}
\begin{itemize}
\item We cannot connect a large number of nodes to a single switch, so
  the topology of the network becomes important.
\begin{itemize}
\item Ring
\item Tree and fat tree
\item Two or higher dimensional mesh and torus
\item Hypercube
\item FFT (butterfly)
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Fat Tree}
\centerline{\pgfimage[width=0.9\textheight]{fat-tree}}
\footnote{\url{http://clusterdesign.org/wp-content/uploads/2012/02/fat_tree_varying_ports-600x365.png}}
\end{frame}

\begin{frame}
\frametitle{Mesh and Torus}
\centerline{\pgfimage[width=0.9\textheight]{mesh-torus.jpg}}
\footnote{\url{http://ars.els-cdn.com/content/image/1-s2.0-S1383762107000495-gr2.jpg}}
\end{frame}

\begin{frame}
\frametitle{FFT}
\centerline{\pgfimage[width=0.8\textheight]{FFT}}
\footnote{\url{http://cnx.org/content/m16352/latest/File0046.png}}
\end{frame}

\begin{frame}
\frametitle{Trend}
\begin{itemize}
\item The most popular topology appears to be fat tree.
\begin{itemize}
\item For example, the Tianhe 2 cluster connects all its processors
  with 13 switches as fat tree.
\end{itemize}
\item Two or higher dimension torus are also popular.
\begin{itemize}
\item The Tofu network of K-computer is a six dimensional torus.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item What is the difference between torus and a mesh?
\end{itemize}
\end{frame}

\subsection{Massively Parallel Computer}

\begin{frame}
\frametitle{Massively Parallel Computer}
\begin{itemize}
\item A computer cluster consists of a set of {\em tightly} connected
  {\em high performance} computers that work together so that in many
  respects they can be viewed as a single system.
\item Tightly connected means they connected by {\em fast}
  network.
\item The performance is paramount and usually achieved by
  aggregation of a large number of processors.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Top 500}
\begin{itemize}
\item The TOP500 project ranks and details the 500 most powerful
  (non-distributed) computer systems in the world.
\item The project aims to provide a reliable basis for tracking and
  detecting trends in high-performance computing and bases rankings on
  HPL, a portable implementation of the High-Performance LINPACK
  benchmark written in Fortran for distributed-memory computers.\footnote{\url{http://top500.org/}}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Top 500}
\centerline{\pgfimage[height=0.6\textheight]{800px-Top500_supercomputers_performance_development.svg}}
\url{https://commons.wikimedia.org/wiki/File:Top500_supercomputers_performance_development.svg}
\end{frame}

\begin{frame}
\frametitle{K computer}
\begin{itemize}
\item The K computer -- named for the Japanese word ``kei'' (京),
  meaning ten quadrillions ($10^{16}$).
\item A supercomputer manufactured by Fujitsu, currently installed at
  the RIKEN Advanced Institute for Computational Science campus in
  Kobe, Japan.
\item In June 2011, TOP500 ranked K the world's fastest supercomputer,
  with a rating of over eight petaflops, and in November 2011, K became
  the first computer to top 10 petaflops.\footnote{\url{http://en.wikipedia.org/wiki/K_computer}}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{K Computer}
\centerline{\pgfimage[width=\textwidth]{k-computer}}
\footnote{\url{http://cdn0.sbnation.com/entry_photo_images/2197300/k-computer_large_verge_medium_landscape.jpg}}
\end{frame}

\begin{frame}
\frametitle{Configuration}
\begin{itemize}
\item 864 cabinets, 88,128 SPARC64 VIIIfx processors, over 640,000 cores.
\item A proprietary six-dimensional torus interconnect called Tofu.
\item A two-level local/global file system with parallel/distributed
  functions, which provides users with an automatic staging function for
  moving files between global and local file systems.
\item Linux operating system.
\item 9.89 MW -- the equivalent of almost 10,000 suburban homes.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{K Computer}
\centerline{\pgfimage[width=0.8\textwidth]{k-computer-rack}}
\footnote{\url{http://cdn-static.zdnet.com/i/story/30/40/093162/k-computer-riken-4.jpg}}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item What is the difference between supercomputer K-computer and Fugaku?
\end{itemize}
\end{frame}


\subsection{Cluster}

\begin{frame}
\frametitle{Cluster}
\begin{itemize}
\item A computer cluster consists of a set of {\em loosely} connected
  computers that work together so that in many respects they can be
  viewed as a single system.
\item Loosely connected means they are {\em not} connected by fast
  network.
\item An economical alternative to those who cannot afford expensive
  parallel computers.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{TrendMicro Cluster}
\begin{itemize}
\item A cluster donated by TrendMicro for the Cloud Computing Program.
\item Loosely connected means they are not connected by fast network.
\item A economical alternative to those who cannot afford expensive
  parallel computers.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Configuration}
\begin{itemize}
\item 1 cabinet, 15 Intel X5570 quad-core processors, 120 cores.
\item It has a standard Ethernet switch.
\item Gluster file system.
\item Roystonea operating system, developed by the Parallel and Distributed Processing Laboratory, Department of Computer Science and Information Engineering, National Taiwan University.
\item Never made it to top 500.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{TrendMicro Cluster}
\centerline{\pgfimage[height=0.8\textheight]{trend-2}}
\end{frame}

\begin{frame}
\frametitle{TrendMicro Cluster}
\centerline{\pgfimage[height=0.8\textheight]{trend-1}}
\end{frame}

\begin{frame}
\begin{itemize}
\item We (the Parallel and Distributed Processing Laboratory) learn
  many things in building Roystonea for the Trend cluster.
\item We learned network virtual machine management, virtual machines deployment, network management for the cluster, distributed file system for the cluster, distributed database, and NoSQL database for a cloud system.
\item We built a cloud OS called ``Roystonea'' to manage the Trend cluster.
\item These experiences later enable us to take on more ambitious projects, like optimizing the billing system of Chunghwa Telecommunication.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item What does the name ``Roystonea'' come from?
\end{itemize}
\end{frame}

\section{Flynn's Taxonomy}

\begin{frame}
\frametitle{Flynn's Taxonomy}
\begin{itemize}
\item Single instruction stream and multiple instruction stream.
\item Single data stream and multiple data stream.
\item We have four combinations -- SISD, SIMD, MISD, and MIMD.
\end{itemize}
\end{frame}

\subsection{SISD}

\begin{frame}
\frametitle{SISD}
\begin{itemize}
\item SISD (single instruction, single data) 
\item A computer architecture in which a single uniprocessor executes
  a single instruction stream to operate on a data stream.
\item Standard von Neumann architecture.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{SISD}
\centerline{\pgfimage[height=0.6\textheight]{SISD}}
\footnote{\url{http://en.wikipedia.org/wiki/SISD}}
\end{frame}

\begin{frame}
\frametitle{Old School}
\begin{itemize}
\item This is how we have been doing -- sequential programming.
\item The compiler for sequential programming is quite mature and can
  transform source program into efficient binaries.
\item System support for sequential programming, e.g., system call,
  user library, debugging are also very useful.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Give an example of SISD machine.
\end{itemize}
\end{frame}

\subsection{SIMD}

\begin{frame}
\frametitle{SIMD}
\begin{itemize}
\item Single instruction, multiple data (SIMD) 
\item A computer architecture in which multiple processors execute a
  single instruction stream to operate on multiple data streams.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{SIMD}
\centerline{\pgfimage[height=0.6\textheight]{SIMD}}
\footnote{\url{http://en.wikipedia.org/wiki/SIMD}}
\end{frame}

\begin{frame}
\frametitle{SIMD}
\begin{itemize}
\item SIMD is strongly related to data-parallel programming since it applies the same instruction to different data and achieves high performance by data parallelism.
\item The architecture of GPUs follows the SIMD model -- the host issues the same command to all processors to process a large amount of data simultaneously.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Give an example of SIMD machine.
\end{itemize}
\end{frame}


\subsection{MIMD}

\begin{frame}
\frametitle{MIMD}
\begin{itemize}
\item Multiple instruction, multiple data (MIMD) 
\item A computer architecture in which multiple processors execute
  multiple instruction streams to operate on multiple data streams.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{MIMD}
\centerline{\pgfimage[height=0.6\textheight]{MIMD}}
\footnote{\url{http://en.wikipedia.org/wiki/MIMD}}
\end{frame}

\begin{frame}
\frametitle{MIMD}
\begin{itemize}
\item Strongly related to functional parallelism since different
  processors execute different instructions on different data.
\item The instructions are different because they are from different
  tasks in a wavefront, i.e., tasks that can run in parallel.
\item The data are different because they are for different tasks.
\item Most multicomputers support the MIMD computation model -- computers work independently and synchronize themselves when necessary.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Give an example of MIMD machine.
\end{itemize}
\end{frame}

\subsection{MISD}

\begin{frame}
\huge Wait! You forgot MISD!
\end{frame}

\begin{frame}
\frametitle{MISD}
\begin{itemize}
\item Multiple instruction, single data (MISD) 
\item A computer architecture in which multiple processors execute
  multiple instruction streams to operate on a single data stream.
\item Does it make sense to you?
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{MISD}
\centerline{\pgfimage[height=0.6\textheight]{MISD}}
\footnote{\url{http://en.wikipedia.org/wiki/MISD}}
\end{frame}

\begin{frame}
\frametitle{Fault Tolerance}
\begin{itemize}
\item If we consider MISD as multiple copies of SISD, it can tolerate faults.
\item The same computation is repeated multiple times by different processors so that at least they can deliver the results even if some of them fail.
\item For example, in Google MapReduce computation, certain tasks are duplicated exactly for fault tolerance and performance improvement.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Give an example of MIMD machine.
\end{itemize}
\end{frame}

\end{CJK}
\end{document}
